
<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<title>Overparametrized SSL</title>
<link href="style.css" rel="stylesheet">
<script type="text/javascript" src="./DreamBooth_files/jquery.mlens-1.0.min.js"></script> 
<script type="text/javascript" src="./DreamBooth_files/jquery.js"></script>
<script type="text/javascript" src="./DreamBooth_files/video-comparison.js"></script>
<script type="module" src="https://ajax.googleapis.com/ajax/libs/model-viewer/3.0.1/model-viewer.min.js"></script>
<!-- <script type="module" src="https://unpkg.com/@google/model-viewer@2.0.1/dist/model-viewer.min.js"></script> -->

<!-- Google tag (gtag.js) -->
<!-- <script async src="https://www.googletagmanager.com/gtag/js?id=G-PWQ7C72CGK"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-PWQ7C72CGK');
</script> -->

<meta charset="utf-8">
<!-- <meta name="viewport" content="width=device-width, initial-scale=1.0, shrink-to-fit=no"> -->
<meta name="viewport" content="width=1000; user-scalable=0;" />
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/4.5.0/css/bootstrap.min.css">
<link href='https://fonts.googleapis.com/css?family=Source+Sans+Pro:300,400,500,600' rel='stylesheet' type='text/css'>
<link rel="stylesheet" href="/assets/css/Highlight-Clean.css">
<link rel="stylesheet" href="/assets/css/styles.css">

<link rel="apple-touch-icon" sizes="180x180" href="favicon.png">
<link rel="icon" type="image/png" sizes="32x32" href="favicon.png">
<link rel="icon" type="image/png" sizes="16x16" href="favicon.png">
<link rel="icon" type="image/png" href="favicon.png">
<link rel="manifest" href="/site.webmanifest">

<meta property="og:site_name" content="O"/>
<meta property="og:type" content="video.other" />
<meta property="og:title" content=">On the Varied Faces of Overparameterization 
in Supervised and Self-Supervised Learning" />
<meta property="og:description" content=">On the Varied Faces of Overparameterization 
in Supervised and Self-Supervised Learning" />
<meta property="og:url" content="https://overparameterized-ssl.github.io/" />

<meta property="article:publisher" content="https://overparameterized-ssl.github.io/" />
<meta name="twitter:card" content="summary_large_image" />
<meta name="twitter:title" content="Overparameterized SSL" />
<meta name="twitter:description" content="Overparameterized SSL" />
<meta name="twitter:url" content="https://overparameterized-ssl.github.io/" />
    <!-- <meta name="twitter:site" content="" /> -->



<style>
  * {
    box-sizing: border-box;
  }
  
  #video-compare-container {
	display: inline-block;
	line-height: 0;
	position: relative;
	width: 100%;
	padding-top: 42.3%;
}
#video-compare-container > video {
	width: 100%;
	position: absolute;
	top: 0;
	height: 100%;
}
#video-clipper {
	width: 50%;
	position: absolute;
	top: 0;
	bottom: 0;
	overflow: hidden;
}
#video-clipper video {
	width: 200%;
	position: absolute;
	height: 100%;
}
  .column {
    float: left;
    width: 20%;
    padding: 5px;
  }

  .colum4 {
    float: left;
    width: 25%;
    padding: 5px;
  }
  
  /* Clearfix (clear floats) */
  .row::after {
    content: "";
    clear: both;
    display: table;
  }
  </style>
 <style>
  .grid {
   display: flex;
   flex-direction: row;
   padding: 5px;
   align-content: center;
   /* flex-wrap: wrap; */
  }
  .grid > div {
   flex-grow: 1;
   flex-shrink: 1;
   padding: 5px;
   
  }
  .center {
  display: block;
  margin-left: auto;
  margin-right: auto;
  width: 50%;
}
html {
  scroll-behavior: smooth;
}
  </style>
</head>

<body>
<div class="content">
  <h1><strong>On the Varied Faces of Overparameterization 
    in Supervised and Self-Supervised Learning</strong></h1>
  <p id="authors"><a href="https://www.kth.se/profile/mgamba">Matteo Gamba</a> <a href="https://arnaghosh.github.io/">Arna Ghosh</a> <a href="https://people.eecs.berkeley.edu/~krishna/">Kumar Krishna Agrawal</a> 
  <p id="authors"><a href="https://mila.quebec/en/person/blake-richards/">Blake Richards</a> <a href="https://www.csc.kth.se/~azizpour/">Hossein Azizpour</a><a href="https://www.kth.se/profile/celle">Mårten Björkman</a> <br>
  <br>
  <span style="font-size: 24px">KTH, Sweden &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Mila, Canada &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; UC Berkeley</span></p> 
  </span></p>
  <br>
  <!-- <img src="teaser_fig.gif" class="teaser-gif" style="width:100%;" loop=infinite><br> -->
  <h3 style="text-align:center"><em>Understanding the role of overparameterization under different learning objectives.</em></h3>
    <font size="+2">
          <p style="text-align: center;">
            <a href="https://openreview.net/forum?id=UaCrxyUeyE" target="_blank">[Paper]</a> &nbsp;&nbsp;&nbsp;&nbsp;
            <a href="#bib">[BibTeX]</a>
          </p>
    </font>
</div>
<div class="content">
  <h2 style="text-align:center;">Abstract</h2>
  <p>The quality of the representations learned by neural networks depends on several
    factors, including the loss function, learning algorithm, and model architecture.
    In this work, we use information geometric measures to assess the representation
    quality in a principled manner. We demonstrate that the sensitivity of learned
    representations to input perturbations, measured by the spectral norm of the feature
    Jacobian, provides valuable information about downstream generalization. On
    the other hand, measuring the coefficient of spectral decay observed in the eigenspectrum of feature covariance provides insights into the global representation
    geometry. First, we empirically establish an equivalence between these notions
    of representation quality and show that they are inversely correlated. Second, our
    analysis reveals the varying roles that overparameterization plays in improving
    generalization. Unlike supervised learning, we observe that increasing model width
    leads to higher discriminability and less smoothness in the self-supervised regime.
    Furthermore, we report that there is no observable double descent phenomenon in
    SSL with non-contrastive objectives for commonly used parameterization regimes,
    which opens up new opportunities for tight asymptotic analysis. Taken together,
    our results provide a loss-aware characterization of the different role of overparameterization in supervised and self-supervised learning.</p>
</div>


<div class="content">
  <a id="bib"><h2>BibTex</h2></a>
  <code>
    @inproceedings{gamba2023varied,<br>
    &nbsp;&nbsp;title={On the Varied Faces of Overparameterization in Supervised and Self-Supervised Learning},<br>
    &nbsp;&nbsp;author={Gamba, Matteo and Ghosh, Arna and Agrawal, Kumar Krishna and Richards, Blake Aaron and Azizpour, Hossein and Bj{\"o}rkman, M{\aa}rten},<br>
    &nbsp;&nbsp;booktitle={NeurIPS 2023 Workshop Heavy Tails in Machine Learning},<br>
    &nbsp;&nbsp;year={2023}<br>
    }
  </code>
</div>

<!-- <div class="content" id="acknowledgements">
  <p><strong>Acknowledgements</strong>:
  </p>
</div> -->
</body>
</html>
